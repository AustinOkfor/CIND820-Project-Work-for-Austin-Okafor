---
title: "Final Results and Project Report"
author: "Austin Okafor"
date: '2022-06-28'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


*****************************************************************************

# In this phase, we will be leveraging on the results of the EDA analysis performed in the previous section in performing our analysis.

First, we install and load some packages we will need for this stage of analysis.


```{r Install Packages}

#install.packages("tidyverse")
# install.packages("rpart.plot")
#Install and load class package
#install.packages('class')
#install.packages("caret", dependencies = c("Depends", "Suggests"))

#library(tidyverse)
library(caret)
library(class)
library(dplyr)
library(readr)
library(rpart)
library(rpart.plot)
library(tictoc)

```


Now, leveraging our previous knowledge of the data from the Descriptive Statistics analysis conducted in our last work, we will load the dataset and assign column types automatically. We will however not change any data value.

```{r Load Data an assign column types}

# I chose not to alter the data in any way because most of the column types are factors.I felt that changing the original values may distort the true understanding of how the data is distributed, because certain levels will be lost.

# Also, due to the number of columns, it was very tedious trying to obtain the column types and ensuring no error was made during assignment. I had to redo this steps several times to ensure accuracy.

# read csv file
dataframe <- 
  read_delim(
    file = "developmental_services_survey_dataset_0.csv",
    col_types = "iffffccfccfffffcfffffffffffffffffffffffffffffcccccnncccccccfcccfccccffcccccf",
    delim=",")


# Reading the first few lines of the dataset
head(dataframe)

```


# Decision Tree Classification Algorithm

Referencing the outputs of SmartEDA and DataExplorer for feature selection, I will now create a subset of the data using only the columns of high and medium importance. This will then be used for the modeling stage of our analysis in this project.


Creating Subset dataframe using columns with High and Median importance features, and adding labels for variables in services.

```{r Subset for Classification}

# Create a subset of the data and substitute labels for services

subset_Data <- dataframe  %>% 
  dplyr::select (c("SERVICES","PR_QL1","C_QE2_2","C_QRB1_1","C_QRB1_2","C_QS2_3","C_QRB2","C_QS1","C_QS3","C_QRB1_3","C_QS2_1","PR_AGE1","PR_QL1A_1","PR_QL1A_2","PR_QL1A_3","PR_QL1A_4","PR_QM1C","PR_QO1_C_COMBINE","PR_QQ")) %>%
  mutate(SERVICES = factor(SERVICES, levels = c(1, 2, 3), labels = c('Residential', 'Non-residential', 'Both'))) 


head(subset_Data)

str(subset_Data)

```


Prepare data for modeling by splitting the dataset into training and test sets.

```{r split into training and test sets}
# create train set and test set

set.seed(525)
train_index <- sample(1:nrow(subset_Data), 0.8 * nrow(subset_Data))
subset_train <- subset_Data[train_index,]
subset_test <- subset_Data[-train_index,]

# check dimension of train and test set to confirm accuracy of split
dim(subset_train)

dim(subset_test)
```



Since our dependent variable, services, has more than 2 possible outcomes,a Multi-class classification algorithm is required.


# Using Classification Model: 
 
We will use rpart.plot for the classification procedure an then proceed to build the model using class

```{r}

fit <- rpart(SERVICES~., data = subset_train, method = 'class')
rpart.plot(fit, extra = "auto")

```

Let us test the model by making a prediction

```{r prediction}

predict_services <-predict(fit, subset_test, type = 'class')

table_Dss <- table(subset_test$SERVICES, predict_services)

table_Dss

```


# Using the Logistics Regression algorithm to predict the Services

Since Linear regression is useful for binary classification, we will need to prepare the data aresh for use regression analysis.

To achieve this, I will create a new column that groups the data into people who use both residential and non-residential services, versus those who only use either. Regression analysis will then be done on this.

We will now load the data and create a subset for classification based on our previous analysis results.

```{r}

new_DF <- read.csv("developmental_services_survey_dataset_0.csv", 
                       header = TRUE, 
                       stringsAsFactors = F) 
                       


nw_DF <- new_DF  %>% 
  dplyr::select (c("SERVICES","PR_QL1","C_QE2_2","C_QRB1_1","C_QRB1_2","C_QS2_3","C_QRB2","C_QS1","C_QS3","C_QRB1_3","C_QS2_1","PR_AGE1","PR_QL1A_1","PR_QL1A_2","PR_QL1A_3","PR_QL1A_4","PR_QM1C","PR_QO1_C_COMBINE","PR_QQ"))


str(nw_DF)

```

Creating a new column called Either_Or_Both to separate patients who used both residential and non-residential services vs thos who only used one of them.

```{r}

# First set the services variables to character
nw_DF$SERVICES <- as.character(nw_DF$SERVICES)

# Create a new column grouping residential and non-residential as "0"
newDF <- nw_DF %>%
    mutate(Either_Or_Both = case_when(
    endsWith(SERVICES, "1") ~ "0",
    endsWith(SERVICES, "2") ~ "0",
    endsWith(SERVICES, "3") ~ "1"
    ))

str(newDF)
```



Remove the SERVICES column as this is no onger needed for our analysis and change PR_QQ to nmeric data

```{r}

# Remove the first column 

newDF <- newDF[ ,-1]

# Changing PR_QQ to numeric data
newDF$PR_QQ <- as.numeric(gsub(",","",newDF$PR_QQ))

# Changing PR_QQ to numeric data
newDF$Either_Or_Both <- as.numeric(gsub(",","",newDF$Either_Or_Both))


str(newDF)

```



Prepare data for modeling by splitting the dataset into training and test sets.

```{r Create Test and Train set}

set.seed(535)
train_newDF <- sample(1:nrow(newDF), 0.8 * nrow(newDF))
newDF_train <- newDF[train_newDF,]
newDF_test <- newDF[-train_newDF,]

```


Remove the class column, Either_Or_Both, from our training and test datasets

```{r Remove Either_Or_Both column}

train.setDF <- newDF_train[-19]
test.setDF <- newDF_test[-19]

str(train.setDF)
str(test.setDF)

```


Creating our Logistic Regression model

```{r LR Model}

glm_model <- glm(Either_Or_Both ~.,newDF_train, family = "binomial")

summary(glm_model)

# From the output of the analysis, we notice that not all variables are significant for prediction, C_QRB2, C_QS1, C_QRB1_3, PR_QM1C, and PR_QO1_C_COMBINE are highly significant, while C_QS2_3 and C_QS3 and less significant in that order.

# In the next phase of my analysis, I will be creating another model using only the significant variables. We will also be running the anova function to analyze the table of deviance.

```


Evaluating the model

```{r}
predict_glm_model <-predict(glm_model, newDF_test, type = "response")

predicted_class <- ifelse(predict_glm_model>=0.5, 1, 0)

table_glm <- table(Actual=newDF_test$Either_Or_Both, Predicted=predicted_class)

table_class <- table(Actual=newDF_test$Either_Or_Both, Predicted=predict_glm_model)

table_glm

```

Confusion Matrix and Statistics

```{r}

confusionMatrix(table_glm)

```



## Checking efficiency by measuring the time for train and test

Efficiency of the Logistic Regression model

```{r Compute LM Runtimes}

tic("Total_glm")
tic("glm_train")
glm_model <- glm(Either_Or_Both ~.,newDF_train, family = "binomial")
toc()
tic("glm_test")
predict_glm_model <-predict(glm_model, newDF_test, type = "response")
toc()
toc()

```



# Recomputing Decision Tree model using the new dependent variables


```{r}

fit2 <- rpart(Either_Or_Both~., data = newDF_train, method = 'class')
rpart.plot(fit2, extra = "auto")


```


```{r}

predict_class_model <-predict(fit2, newDF_test, type = 'class')

table_class <- table(Actual_=newDF_test$Either_Or_Both, Predicted_=predict_class_model)

table_class

```


Confusion Matrix and Statistics

```{r}

confusionMatrix(table_class)

```

## Checking efficiency by measuring the time for train and test

Efficiency of the Decision Tree model

```{r Compute DT Runtimes}

tic("Total_dTree")
tic("dTree_train")
fit2 <- rpart(Either_Or_Both~., data = newDF_train, method = 'class')
toc()
tic("dTree_test")
predict_class_model <-predict(fit2, newDF_test, type = 'class')
toc()
toc()


```


## Now let us attempt to generate the decision tree using Random Forest method.

First we install and load the pachage needed.

```{r}
#install.packages("randomForest")

library(randomForest)
```


Now we fit the model and review it.

### Note that all attempts to generate a Classification type model failed, displaying "Error: Cannot allocate vector of size 5.4Gb", before terminating the process. The below code only generates a Regression model. consequently, we will not be exploring this further.


```{r}
# This is the correct code to use for Classification but gives an error

#fit_RF <- randomForest(Either_Or_Both~., data = newDF_train, importance = TRUE, proximity = TRUE)


fit_RF <- randomForest(Either_Or_Both~., data = newDF_train)

print(fit_RF)

```



# Using k-Nearest Neighbors (KNN) Algorithm

If the data is not normalized it will lead to a biased outcome. To normalize, we first create a Normalization function. 

```{r Normalize functio}

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

```



Then we normalize the data, but not before removing the dependent variable from the dataset.

```{r Normalize Data}

newDF.n <- newDF

newDF.norm <- as.data.frame(lapply(newDF.n[,1:18], normalize))

newDF.norm <- cbind(newDF.n$Either_Or_Both, newDF.norm)

head(newDF.norm)

```


After normalizing, we split the data set into training and testing data set.

```{r Creating Test and Train set}

set.seed(545)
index <- sample(1:nrow(newDF.norm), 0.8 * nrow(newDF.norm))
DF.norm_train <- newDF.norm[index,]
DF.norm_test <- newDF.norm[-index,]


head(DF.norm_train)

```


We now create the train and test labels for comparing results by extracting the dependent variable, Either_Or_Both, from the new dataframe.

```{r New tata with Either_Or_Both}

DF.n_train_labels <- DF.norm_train[,1]
DF.n_test_labels <- DF.norm_test[,1]


```


Calculate the square root of the number of observations in the training data set to use in determining the optimal value of 'K' in the KNN model.

```{r}

round(sqrt(NROW(DF.n_train_labels)))

```

Creating the KNN algorithm model and using the model to make a prediction.

```{r kNNModel Generation and Prediction}

DF.n_test_pred <- knn(train = DF.norm_train[,2:19],
test = DF.norm_test[,2:19],
cl = DF.norm_train[,1], k=164)

head(DF.n_test_pred)

```


Evaluating the model

```{r kNN Model Evaluation}

table_KNN <- table(Actual=DF.n_test_labels, Predicted=DF.n_test_pred)

table_KNN

```

Confusion Matrix and Statistics

```{r}

confusionMatrix(table_KNN)

```

## Checking efficiency by measuring the time for train and test

Efficiecy of the  k-Nearest Neighbors model

```{r Compute KNN Runtimes}

tic("Total_KNN")
tic("KNN_train")
DF.n_test_pred <- knn(train = DF.norm_train[,2:19], test = DF.norm_test[,2:19], cl = DF.norm_train[,1], k=164)
toc()
tic("KNN_test")
table_KNN <- table(Actual=DF.n_test_labels, Predicted=DF.n_test_pred)
toc()
toc()

```


# Performing Cross-Validation on the Three Procedures

## We will now compare the performance of the algorithms using Repeated K-fold cross-validation method


Let's define a train control model with K = 10 with 5 repeats

```{r 10-fold model with 5 repeats}

# defining training control as repeated cross-validation and value of K is 10 and repetition is 5 times

set.seed(120)

train_control <- trainControl(method = "repeatedcv",
                            number = 10, repeats = 5)

```



## Repeated K-fold cross-validation - Decision Tree Model

```{r Repeated K-fold - Decision Tree}

DT_model <- train(Either_Or_Both~., data = newDF_train,
               trControl = train_control,
               method = "rpart")

print(DT_model)

```



## Repeated K-fold cross-validation - Linear Regression Model

```{r Repeated K-fold - Linear Regression}

LM_model <- train(Either_Or_Both~., data = newDF_train,
               trControl = train_control,
               method = "lm")

print(LM_model)

```


## Repeated K-fold cross-validation - k-Nearest Neighbors Model

```{r Repeated K-fold - k-Nearest Neighbors}

train = DF.norm_train[,2:19]
cl = DF.norm_train[,1]

KNN_model = train(x = train,
                y = cl,
                method = "knn")
                trControl = train_control

print(KNN_model)

```



# Comparing Accuracy, Sensitivity, Specificity, Rsquared, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE)


GLM -   Accuracy  : 0.9143 
      Sensitivity : 0.8802
      Specificity : 0.9462
          RMSE    : 0.2872896  
         Rsquared : 0.6669755  
              MAE : 0.2094923

      
DTree -  Accuracy : 0.9252   
      Sensitivity : 0.8717
      Specificity : 0.9806
          RMSE    : 0.3045250    
         Rsquared : 0.6248430  
              MAE : 0.1854077
      
      
KNN -    Accuracy : 0.9128     
      Sensitivity : 0.8622
      Specificity : 0.9645
             RMSE : 0.2428542  
         Rsquared : 0.7637671  
              MAE : 0.09968792

